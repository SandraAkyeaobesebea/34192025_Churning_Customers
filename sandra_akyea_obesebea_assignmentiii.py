# -*- coding: utf-8 -*-
"""Sandra.Akyea-Obesebea_AssignmentIII.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pb42MAqjzUClb-VfuaSIbyTi3chR53op

# Mounted my google drive
"""

import pandas as pd
import numpy as np
import joblib
import seaborn as sns
from keras.layers import Dropout
from keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from google.colab import drive
drive.mount('/content/drive')

"""# Read the dataset named customer_churn

"""

customer_churn=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets_13996_18858_WA_Fn-UseC_-Telco-Customer-Churn.csv')

"""# Display the dataset

"""

customer_churn

"""Identify the columns in the dataset"""

customer_churn.columns

"""#Create a list of columns with more than 30% missing values"""

# Calculate the percentage of missing values in each column
missing_percentage = (customer_churn.isnull().sum() / len(customer_churn)) * 100
# Identify columns with more than 30% missing values
columns_to_drop = missing_percentage[missing_percentage > 30].index

# Drop the identified columns from the DataFrame
customer_churn.drop(columns=columns_to_drop, inplace=True)

"""
# Dropping the customerID"""

customer_churn.drop('customerID', axis = 1, inplace = True)

customer_churn

"""# Change Total Charges to numeric

"""

#customer_churn_copy = customer_churn.copy()
customer_churn['TotalCharges'] = pd.to_numeric(customer_churn['TotalCharges'], errors='coerce')

"""# Checking for missing values"""

customer_churn.isnull().sum()

"""# Fill in missing values"""

customer_churn['TotalCharges'].fillna(customer_churn['TotalCharges'].mean(), inplace=True)

"""Check for missing values again"""

customer_churn.isnull().sum()

"""# Identify  the numeric columns"""

numeric_columns = customer_churn.select_dtypes(include=['number'])
numeric_columns

"""# Identify the textual columns"""

text_columns= customer_churn.select_dtypes(include=['object'])
text_columns

# Creating a LabelEncoder instance
label_encoder = LabelEncoder()
# Iterating through the textual columns and encode them
for column in text_columns:
    customer_churn[column] = label_encoder.fit_transform(customer_churn[column])

customer_churn.head()

"""# Checking the correlation of each feature"""

correlation_matrix = customer_churn.corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""# Setting the correlation limit and Filtering features based on the correlation limit"""

correlation_limit = 0.03

target_correlation = correlation_matrix['Churn']

selected_features = target_correlation[abs(target_correlation) > correlation_limit].index

selected_features

sns.pairplot(data=customer_churn, vars=selected_features, hue='Churn')
plt.title('Pair Plot of Selected Features by Target')
plt.show()

"""# Visualization of  selected features

---


"""

for feature in selected_features:
    sns.boxplot(data=customer_churn, x='Churn', y=feature)
    plt.title(f'Box Plot of {feature}')
    plt.show()

for feature in selected_features:
    sns.countplot(data=customer_churn, x=feature, hue='Churn')
    plt.title(f'Count Plot of {feature} by Target')
    plt.show()

    # In the first distribution diagram the SeniorCitizen is less likely to churn
    # In the second distribution diagram the customers without partners are likely to churn more than customers with partners.
    # In the third distribution diagram customers with no Techsupport are likely to churn more than customers with internet services which are less likely to churn.

# Split data into features (X) and target variable (y)
X = customer_churn[selected_features].drop('Churn', axis=1)
y = customer_churn['Churn']

# Perform train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the input layer
input_layer = Input(shape=(X_train.shape[1],))

# Define the hidden layers
hidden_layer_1 = Dense(128, activation='relu')(input_layer)
hidden_layer_2 = Dense(64, activation='relu')(hidden_layer_1)

# Define the output layer
output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

# Define the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model with binary_crossentropy for binary classification
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model on the training data
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)

# Evaluate the model on the test data
y_pred = model.predict(X_test_scaled)
y_pred_binary = (y_pred > 0.5).astype(int) # Convert probabilities to binary predictions
accuracy = accuracy_score(y_test, y_pred_binary)
roc_auc = roc_auc_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')
print(f'ROC AUC: {roc_auc:.4f}')

!pip install keras==2.12.0
from keras.wrappers.scikit_learn import KerasClassifier

# Define a function to create the model
def create_model(optimizer='adam', hidden_layer_1=128, hidden_layer_2=64):
    input_layer = Input(shape=(X_train_scaled.shape[1],))
    hidden_layer_1 = Dense(hidden_layer_1, activation='relu')(input_layer)
    hidden_layer_2 = Dense(hidden_layer_2, activation='relu')(hidden_layer_1)
    output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create KerasClassifier with create_model function
model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, validation_split=0.1, verbose=0)

# Define hyperparameters to search
param_grid = {
    'optimizer': ['adam', 'rmsprop'],
    'hidden_layer_1': [64, 128, 256],
    'hidden_layer_2': [32, 64, 128]
}

# Use GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
grid_result = grid_search.fit(X_train_scaled, y_train)

# Print the best parameters and results
print(f'Best Parameters: {grid_result.best_params_}')
print(f'Best Accuracy: {grid_result.best_score_:.4f}')

# Evaluate the best model on the test data
best_model = grid_result.best_estimator_
y_pred = best_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')
print(f'ROC AUC: {roc_auc:.4f}')

from keras.models import save_model
# Save the scaler using joblib
joblib.dump(scaler, 'scaler.pkl')
best_model.model.save('best_model.h5')